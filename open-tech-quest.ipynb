{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c06e224-7916-4b95-ab14-37ebd2e2b761",
   "metadata": {},
   "source": [
    "# Testing the Model Deployment with your WebCam ðŸ“¹\n",
    "\n",
    "After deploying the model using RHODS Model Serving, we'd like to test the model deployment by sending images to the model server for real-time inference.\n",
    "\n",
    "In this notebook we'll review how to consume the model through the RHODS Model Server.\n",
    "\n",
    "For testing the model deployment, our test script needs to know the address of the model server. Let's insert the **inference endpoint** that the RHODS Dashboard provides for the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7ffad01-a4cc-4d96-b725-309655636768",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_url = 'https://model-team-1-ai.apps.cluster-djsg4.djsg4.sandbox703.opentlc.com/v2/models/model/infer'\n",
    "token = 'eyJhbGciOiJSUzI1NiIsImtpZCI6InZ2NFZUTVRJS2wwYnA5RXVhcHY5UWJBSzFmOUpaY2pxODhPejhhck5WWkUifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJ0ZWFtLTEtYWkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiZGVmYXVsdC1uYW1lLW1vZGVsLXNlcnZlci1zYSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJtb2RlbC1zZXJ2ZXItc2EiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmYWI5NDVmOC1hMGI0LTRkYjEtYTFkMS0zMjZhZGFiYWRmOWQiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6dGVhbS0xLWFpOm1vZGVsLXNlcnZlci1zYSJ9.SXovPphySt-8pBsmNJwCn4jsPVlx7yww0gSvqk0Ym6KYus2fdBwwbbrdLPDwKVNC21sDIUZegyB-5I2ms0acooEZCdHU6y3jRQAp_ml8nridVIZzNZqWRTCx3toS3z7gV8x8A_daRDUVCVariFtVizhd6Loz78iUDY_jUWWbJT6MnKMM65LBXvsQqRjFWiIZTSe1x21K4FzdNjKvVKlcTfCSYdfp3Iuc7r5VKcat0JxBO7Vp8yFDffuQHihBCd_UsAMpvN7yRZEj2GGfGbV9yh-R-Y5aZl_1Tlw-34xtXClp_odHZ4Iu11DBRrL4__auzvl4Sf-zB4FLt3eRG22A3YL6Z2NwrDZ4z7q9BrR17z7o99CG-ZWMM2xy7xiTRky69Az0efhM84ZiX9TXyJoraf9hu_znuV9NUpBAoAdiC7ASvg6fHg20kddOdAfOu6EfZPqrbLgY2pOPnotGWoYBb89hoL-0q15QnBfJ-daSu5S_dSzbTTAzmrGzTll1gt3V5anRBmls3MGMyAupy1Oz3AhdppIgJtrm-JmuXsIet_7GJxrOxAqUjtd3rto9B1up_AFHWuxScTU4CxuKJd90d2ssbYLZrJv_WaTtPbF0hDIuhZ2yN7XJwhB-ov6ZPRQFZUUy6fpvgsuEWp-zksgmP29IBE8lcpUp1ICrV2BlgbY'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c949b-9fca-4629-b18d-487fb856bfc8",
   "metadata": {},
   "source": [
    "We'll start by importing the preprocessing and rendering functions that we have worked with in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd48db30-205c-4458-8f84-2e55599db6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from jupyter_ui_poll import ui_events\n",
    "import ipywidgets as widgets\n",
    "from requests import post\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from preprocessing import preprocess_image_file,_transform\n",
    "\n",
    "from object_detection import postprocess\n",
    "from object_rendering import draw_boxes\n",
    "import yaml\n",
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "from ipywebrtc import CameraStream, ImageRecorder\n",
    "camera = CameraStream(constraints=\n",
    "                      {'facing_mode': 'user',\n",
    "                       'audio': False,\n",
    "                       'video': { 'width': 640, 'height': 480 }\n",
    "                       })\n",
    "# camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29759839-1a91-470c-a65b-701f84011281",
   "metadata": {},
   "source": [
    "## Lets take a Picture\n",
    "\n",
    "with the little ðŸ“· icon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d4fa148-ae98-46d9-9309-a17e43e8a887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88b0815dd5e4b9eb16fcfe76612a57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ImageRecorder(image=Image(value=b''), recording=True, stream=CameraStream(constraints={'facing_mode': 'user', â€¦"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_recorder = ImageRecorder(stream=camera)\n",
    "image_recorder.recording = True\n",
    "image_recorder.autosave = False\n",
    "image_recorder.download()\n",
    "image_recorder.image.height\n",
    "image_recorder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0990e-7705-4438-81b6-bfad6c67d86b",
   "metadata": {},
   "source": [
    "## Click \"Continue\" button to continue with your picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65746204-0742-4209-af8d-569dc6d67bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "button = widgets.Button(description = \"Continue\")\n",
    "output = widgets.Output()\n",
    "display(button, output)\n",
    "\n",
    "clicked = False\n",
    "def on_button_clicked(b):\n",
    "    global clicked\n",
    "    clicked = True\n",
    "    with output:\n",
    "        image_recorder.save('webcam')\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "with ui_events() as poll:\n",
    "    while not clicked:\n",
    "        poll(10) # poll queued UI events including button\n",
    "        time.sleep(1) # wait for 1 second before checking again\n",
    "        \n",
    "print(\"Let's proceed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04340078-0a81-4f97-817e-ca2e03e59c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c78714-460e-40c7-b47d-2685f4b668b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'webcam.png'\n",
    "transformed_image, scaling, padding = preprocess_image_file(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2cb9cf-48c6-438c-a776-e06d013dcd0c",
   "metadata": {},
   "source": [
    "We'll now need to package the preprocessed image into a format that the model server can consume. RHODS Model Serving implements a generic prediction interface that allows to query the typical model formats through the HTTP POST method using a JSON request body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cdc026-6119-4749-ab98-44d1828e64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(image):\n",
    "    payload = {\n",
    "        'inputs': [\n",
    "            {\n",
    "                'name': 'images',\n",
    "                'shape': [1, 3, 640, 640],\n",
    "                'datatype': 'FP32',\n",
    "                'data': image.flatten().tolist(),\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752a32e3-5202-480c-bac7-3a12706ca963",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = serialize(transformed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d859e1-f9d1-4801-be38-8db71cf8e31e",
   "metadata": {},
   "source": [
    "Let's now send the serialized cat image to the model server. The inference results will also be returned in a generic JSON structure, which we can unpack straightaway. We'll also apply the post-processing function we defined in the previous notebook to extract the familiar object properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3eb6c3-43d0-4fbb-a830-286c118218fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(payload, prediction_url, classes_count, token=''):\n",
    "    headers = {'Authorization': f'Bearer {token}'} if token else {}\n",
    "    raw_response = post(prediction_url, json=payload, headers=headers)\n",
    "    try:\n",
    "        response = raw_response.json()\n",
    "    except:\n",
    "        print(f'Failed to deserialize service response.\\n'\n",
    "              f'Status code: {raw_response.status_code}\\n'\n",
    "              f'Response body: {raw_response.text}')\n",
    "    try:\n",
    "        model_output = response['outputs']\n",
    "    except:\n",
    "        print(f'Failed to extract model output from service response.\\n'\n",
    "              f'Service response: {response}')\n",
    "    unpacked_output = _unpack(model_output, classes_count)\n",
    "    return unpacked_output\n",
    "\n",
    "\n",
    "def _unpack(model_output, classes_count):\n",
    "    arr = np.array(model_output[0]['data'])\n",
    "    # Get the response data as a NumPy Array\n",
    "\n",
    "    output = torch.tensor(arr)  # Create a tensor from array\n",
    "    prediction_columns_number = 5 + classes_count\n",
    "    # Model returns model returns [xywh, conf, class0, class1, ...]\n",
    "\n",
    "    output = output.reshape(\n",
    "        1,\n",
    "        int(int(output.shape[0])/prediction_columns_number),\n",
    "        prediction_columns_number\n",
    "    )  # Reshape the flat array prediction\n",
    "\n",
    "    return output\n",
    "\n",
    "def _read_class_labels(configuration_file_path):\n",
    "    with open(configuration_file_path, 'r') as config_file:\n",
    "        config = yaml.load(config_file.read(), Loader=yaml.SafeLoader)\n",
    "\n",
    "    class_labels = config['names']\n",
    "    return class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66568dae-81cd-41cd-80fd-645c39a08755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_labels = _read_class_labels('model-training/configuration.yaml')\n",
    "raw_objects = get_model_response(payload, prediction_url, len(class_labels), token=token)\n",
    "objects = postprocess(raw_objects, conf_thres=0.3)\n",
    "objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4baed58-595b-49ff-86ed-c9921368a1a8",
   "metadata": {},
   "source": [
    "Let's now visualize the result as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96301c5-5198-420e-9ab6-3efd9e8e844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boxes(image_path, objects, scaling, padding, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c64ff-f6d2-4bfa-8417-2885c0b94ee7",
   "metadata": {},
   "source": [
    "We were able to reproduce the object detection example from the previous notebook, so we can consume the deployed model as expected.\n",
    "\n",
    "You can now head over to deploying the object detection application to consume this model in a real-time fashion.\n",
    "\n",
    "After that, we'll explore offline scoring in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
